# ğŸ“° Autonomous News Processing with RAG & CrewAI

## Overview
This project implements a **Retrieval-Augmented Generation (RAG) pipeline** using **CrewAI** to automate news searching, classification, and storage. Given a user query, the system:
1. **Searches the web** using Tavily API.
2. **Optimizes results** using an SEO agent.
3. **Classifies & Parses news** into structured data.
4. **Embeds the news** into ChromaDB for retrieval.
5. **Retrieves relevant documents** for answering queries.
6. **Grades responses** using hallucination and answer grading agents.

## ğŸš€ Features
âœ” **Automated Web Search** (Tavily API)  
âœ” **SEO Optimization** to pick the best source  
âœ” **Classifies & Summarizes News** (LLM-based)  
âœ” **Embeds Data into ChromaDB** for efficient retrieval  
âœ” **Retrieval-Augmented Generation (RAG)** for accurate responses  
âœ” **Grading System** to reduce hallucinations  
âœ” **Scalable & Modular** CrewAI-based architecture  

## ğŸ› ï¸ Tech Stack
- **Python** (Backend Processing)
- **CrewAI** (Multi-agent AI System)
- **LangChain** (LLM & RAG Framework)
- **Tavily API** (Web Search)
- **ChromaDB** (Vector Database for Retrieval)
- **FastAPI** (API Endpoint for Queries)
- **Streamlit** (Frontend UI for Interaction)
- **GroqCloud (DeepSeek-Qwen-32B / Llama-3.3-70B / Mistral-70B)** (LLM)

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ agents.py          # CrewAI Agents (Search, SEO, Retriever, Graders)
â”‚   â”œâ”€â”€ classify_parse.py  # News Classification & Parsing
â”‚   â”œâ”€â”€ embedder.py        # ChromaDB Embedding & Retrieval
â”‚   â”œâ”€â”€ rag_pipeline.py    # RAG Workflow Implementation
â”‚   â”œâ”€â”€ app.py             # FastAPI Endpoints
â”‚   â”œâ”€â”€ frontend.py        # Streamlit Frontend
â”‚   â”œâ”€â”€ utils.py           # Helper Functions (Cleaning, Storage)
â”‚
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ news_dataset.csv   # Preprocessed News Data
â”‚   â”œâ”€â”€ chromadb_index/    # ChromaDB Vector Store
â”‚
â”œâ”€â”€ README.md              # Project Documentation
â”œâ”€â”€ requirements.txt       # Python Dependencies
â”œâ”€â”€ .env                   # API Keys & Configuration
```

## ğŸ—ï¸ Setup & Installation
### 1ï¸âƒ£ Clone Repository
```sh
git clone https://github.com/yourusername/rag-news-pipeline.git
cd rag-news-pipeline
```

### 2ï¸âƒ£ Install Dependencies
```sh
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure API Keys (Tavily, Groq, ChromaDB)
Create a `.env` file and add:
```env
TAVILY_API_KEY=your_tavily_key
GROQ_API_KEY=your_groq_key
CHROMADB_PATH=./data/chromadb_index
```

### 4ï¸âƒ£ Run the FastAPI Backend
```sh
uvicorn src.app:app --reload
```

### 5ï¸âƒ£ Run the Streamlit Frontend
```sh
streamlit run src/frontend.py
```

## ğŸƒ Usage
### ğŸ”¹ **Querying the System** (API)
Send a GET request:
```sh
curl -X GET "http://localhost:8000/query?prompt='Latest updates on AI regulation'"
```

### ğŸ”¹ **Using the Streamlit UI**
1. Open `http://localhost:8501` in your browser.
2. Enter a **news query** or **URL**.
3. View **structured results** with Type, Subtype, Summary, and Source Link.

## ğŸ› ï¸ Future Improvements
- Fine-tuning LLM on domain-specific news data
- Adding **multi-modal** support (video, images)
- Expanding to more **news categories & regions**
- **Fact-checking agent** for misinformation detection

---
**ğŸ“Œ Contribute**: Open issues, submit PRs, and help improve this project!

ğŸ”— **GitHub Repo**: [your-repo-link](https://github.com/yourusername/rag-news-pipeline)
